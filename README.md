# Awesom-AGI Survey Papers

Must-read Papers on Artifical General Intelligence with foundation models.

---

# 📜Content

- [Awesom-AGI Survey Papers](#awesom-agi-survey-papers)
- [📜Content](#content)
  - [1. Introduction](#1-introduction)
  - [2. AGI Internal: Unveiling the Mind of AGI](#2-agi-internal-unveiling-the-mind-of-agi)
    - [2.1 Perception](#21-perception)
    - [2.2 Reasoning](#22-reasoning)
    - [2.3 Memory](#23-memory)
    - [2.4 Metacognition](#24-metacognition)
  - [3. AGI Interface: Connecting the World with AGI](#3-agi-interface-connecting-the-world-with-agi)
    - [3.1 Interfaces to Digital World](#31-interfaces-to-digital-world)
    - [3.2 Interfaces to Physical World](#32-interfaces-to-physical-world)
    - [3.3 Interfaces to Intelligence](#33-interfaces-to-intelligence)
      - [3.3.1 Interfaces to AI Agents](#331-interfaces-to-ai-agents)
      - [3.3.2 Interfaces to Humans](#332-interfaces-to-humans)
  - [4. AGI Systems: Implementing the Mechanism of AGI](#4-agi-systems-implementing-the-mechanism-of-agi)
    - [4.1 System Challenges](#41-system-challenges)
    - [4.2 Scalable Model Architectures](#42-scalable-model-architectures)
    - [4.3 Large-scale Training](#43-large-scale-training)
    - [4.4 Inference Techniques](#44-inference-techniques)
    - [4.5 Cost and Efficiency](#45-cost-and-efficiency)
    - [4.6 Computing Platforms](#46-computing-platforms)
    - [4.7 The Future of AGI Systems](#47-the-future-of-agi-systems)
  - [5. AGI Alignment: Reconciling Needs with AGI](#5-agi-alignment-reconciling-needs-with-agi)
    - [5.1 Expectations of AGI Alignment](#51-expectations-of-agi-alignment)
    - [5.2 AGI Alignment Classifications](#52-agi-alignment-classifications)
    - [5.3 How to Implement: Solutions for Alignment](#53-how-to-implement-solutions-for-alignment)
  - [6. Approach AGI Responsibly](#6-approach-agi-responsibly)
    - [6.1 AI Levels: Charting the Evolution of Artificial Intelligence](#61-ai-levels-charting-the-evolution-of-artificial-intelligence)
      - [6.1.1 AGI Levels](#611-agi-levels)
      - [6.1.2 Constraints and Challenges of Ultimate AGI](#612-constraints-and-challenges-of-ultimate-agi)
      - [6.1.3 How Do We Get to the Next Level of AGI?](#613-how-do-we-get-to-the-next-level-of-agi)
    - [6.2 AGI Evaluation](#62-agi-evaluation)
      - [6.2.1 What Do We Expect from AGI Evaluations](#621-what-do-we-expect-from-agi-evaluations)
      - [6.2.2 Current Evaluation Frameworks and Limitations](#622-current-evaluation-frameworks-and-limitations)
    - [6.3 Potential Ways to Future AGI](#63-potential-ways-to-future-agi)
  - [7. Case Studies](#7-case-studies)
    - [7.1 AI for Science Discovery and Research](#71-ai-for-science-discovery-and-research)
    - [7.2 Generative Visual Intelligence](#72-generative-visual-intelligence)
    - [7.3 World Models](#73-world-models)
    - [7.4 Decentralized LLM](#74-decentralized-llm)
    - [7.5 AI for Coding](#75-ai-for-coding)
    - [7.6 AI for Robotics in Real World Applications](#76-ai-for-robotics-in-real-world-applications)
    - [7.7 Human-AI Collaboration](#77-human-ai-collaboration)
  - [8. Conclusion](#8-conclusion)


## 1. Introduction

## 2. AGI Internal: Unveiling the Mind of AGI
### 2.1 Perception

1. **Experience grounds language** *Bisk, Yonatan, Holtzman, Ari, Thomason, Jesse, Andreas, Jacob, Bengio, Yoshua, Chai, Joyce, Lapata, Mirella, Lazaridou, Angeliki, May, Jonathan, Nisnevich, Aleksandr, others.* arXiv preprint arXiv:2004.10151, 2020. [[abs](https://arxiv.org/abs/2004.10151)]
2. **High-resolution image synthesis with latent diffusion models** *Rombach, Robin, Blattmann, Andreas, Lorenz, Dominik, Esser, Patrick, Ommer, Bj\"o.* Presented at Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. [Link](No URL)
3. **Flamingo: a visual language model for few-shot learning** *Alayrac, Jean-Baptiste, Donahue, Jeff, Luc, Pauline, Miech, Antoine, Barr, Iain, Hasson, Yana, Lenc, Karel, Mensch, Arthur, Millican, Katherine, Reynolds, Malcolm, others.* No journal, 2022.
4. **Sequential Modeling Enables Scalable Learning for Large Vision Models** *Bai, Yutong, Geng, Xinyang, Mangalam, Karttikeya, Bar, Amir, Yuille, Alan, Darrell, Trevor, Malik, Jitendra, Efros, Alexei A.* arXiv preprint arXiv:2312.00785, 2023. [[abs](https://arxiv.org/abs/2312.00785)]
5. **Voyager: An open-ended embodied agent with large language models** *Wang, Guanzhi, Xie, Yuqi, Jiang, Yunfan, Mandlekar, Ajay, Xiao, Chaowei, Zhu, Yuke, Fan, Linxi, Anandkumar, Anima.* arXiv preprint arXiv:2305.16291, 2023. [[abs](https://arxiv.org/abs/2305.16291)]
6. **Generating images with multimodal language models** *Koh, Jing Yu, Fried, Daniel, Salakhutdinov, Ruslan.* arXiv preprint arXiv:2305.17216, 2023. [[abs](https://arxiv.org/abs/2305.17216)]
7. **What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models** *Zhang, Letian, Zhai, Xiaotong, Zhao, Zhongkai, Wen, Xin, Zhao, Bingchen.* Presented at Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [Link](No URL)
8. **Otter: A multi-modal model with in-context instruction tuning** *Li, Bo, Zhang, Yuanhan, Chen, Liangyu, Wang, Jinghao, Yang, Jingkang, Liu, Ziwei.* arXiv preprint arXiv:2305.03726, 2023. [[abs](https://arxiv.org/abs/2305.03726)]
9. **Videochat: Chat-centric video understanding** *Li, KunChang, He, Yinan, Wang, Yi, Li, Yizhuo, Wang, Wenhai, Luo, Ping, Wang, Yali, Wang, Limin, Qiao, Yu.* arXiv preprint arXiv:2305.06355, 2023. [[abs](https://arxiv.org/abs/2305.06355)]
10. **Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents** *Chen, Weize, Su, Yusheng, Zuo, Jingwei, Yang, Cheng, Yuan, Chenfei, Qian, Chen, Chan, Chi-Min, Qin, Yujia, Lu, Yaxi, Xie, Ruobing, others.* arXiv preprint arXiv:2308.10848, 2023. [[abs](https://arxiv.org/abs/2308.10848)]
11. **X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages** *Chen, Feilong, Han, Minglun, Zhao, Haozhi, Zhang, Qingyang, Shi, Jing, Xu, Shuang, Xu, Bo.* arXiv preprint arXiv:2305.04160, 2023. [[abs](https://arxiv.org/abs/2305.04160)]
12. **Cheap and quick: Efficient vision-language instruction tuning for large language models** *Luo, Gen, Zhou, Yiyi, Ren, Tianhe, Chen, Shengxin, Sun, Xiaoshuai, Ji, Rongrong.* arXiv preprint arXiv:2305.15023, 2023. [[abs](https://arxiv.org/abs/2305.15023)]
13. **LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents** *Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li.* arXiv preprint arXiv:2311.05437, 2023. [[abs](https://arxiv.org/abs/2311.05437)]
14. **Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration** *Lyu, Chenyang, Wu, Minghao, Wang, Longyue, Huang, Xinting, Liu, Bingshuai, Du, Zefeng, Shi, Shuming, Tu, Zhaopeng.* arXiv preprint arXiv:2306.09093, 2023. [[abs](https://arxiv.org/abs/2306.09093)]
15. **mplug-owl: Modularization empowers large language models with multimodality** *Ye, Qinghao, Xu, Haiyang, Xu, Guohai, Ye, Jiabo, Yan, Ming, Zhou, Yiyang, Wang, Junyang, Hu, Anwen, Shi, Pengcheng, Shi, Yaya, others.* arXiv preprint arXiv:2304.14178, 2023. [[abs](https://arxiv.org/abs/2304.14178)]
16. **A Survey on Multimodal Large Language Models** *Yin, Shukang, Fu, Chaoyou, Zhao, Sirui, Li, Ke, Sun, Xing, Xu, Tong, Chen, Enhong.* arXiv preprint arXiv:2306.13549, 2023. [[abs](https://arxiv.org/abs/2306.13549)]
17. **Imagebind-llm: Multi-modality instruction tuning** *Han, Jiaming, Zhang, Renrui, Shao, Wenqi, Gao, Peng, Xu, Peng, Xiao, Han, Zhang, Kaipeng, Liu, Chris, Wen, Song, Guo, Ziyu, others.* arXiv preprint arXiv:2309.03905, 2023. [[abs](https://arxiv.org/abs/2309.03905)]
18. **OneLLM: One Framework to Align All Modalities with Language** *Han, Jiaming, Gong, Kaixiong, Zhang, Yiyuan, Wang, Jiaqi, Zhang, Kaipeng, Lin, Dahua, Qiao, Yu, Gao, Peng, Yue, Xiangyu.* arXiv preprint arXiv:2312.03700, 2023. [[abs](https://arxiv.org/abs/2312.03700)]
19. **Planting a seed of vision in large language model** *Ge, Yuying, Ge, Yixiao, Zeng, Ziyun, Wang, Xintao, Shan, Ying.* arXiv preprint arXiv:2307.08041, 2023. [[abs](https://arxiv.org/abs/2307.08041)]
20. **Seggpt: Segmenting everything in context** *Wang, Xinlong, Zhang, Xiaosong, Cao, Yue, Wang, Wen, Shen, Chunhua, Huang, Tiejun.* arXiv preprint arXiv:2304.03284, 2023. [[abs](https://arxiv.org/abs/2304.03284)]
21. **Audiogpt: Understanding and generating speech, music, sound, and talking head** *Huang, Rongjie, Li, Mingze, Yang, Dongchao, Shi, Jiatong, Chang, Xuankai, Ye, Zhenhui, Wu, Yuning, Hong, Zhiqing, Huang, Jiawei, Liu, Jinglin, others.* arXiv preprint arXiv:2304.12995, 2023. [[abs](https://arxiv.org/abs/2304.12995)]
22. **Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges** *Cui, Chenhang, Zhou, Yiyang, Yang, Xinyu, Wu, Shirley, Zhang, Linjun, Zou, James, Yao, Huaxiu.* arXiv preprint arXiv:2311.03287, 2023. [[abs](https://arxiv.org/abs/2311.03287)]
23. **De-Diffusion Makes Text a Strong Cross-Modal Interface** *Wei, Chen, Liu, Chenxi, Qiao, Siyuan, Zhang, Zhishuai, Yuille, Alan, Yu, Jiahui.* arXiv preprint arXiv:2311.00618, 2023. [[abs](https://arxiv.org/abs/2311.00618)]
24. **Pandagpt: One model to instruction-follow them all** *Su, Yixuan, Lan, Tian, Li, Huayang, Xu, Jialu, Wang, Yan, Cai, Deng.* arXiv preprint arXiv:2305.16355, 2023. [[abs](https://arxiv.org/abs/2305.16355)]
25. **On evaluating adversarial robustness of large vision-language models** *Zhao, Yunqing, Pang, Tianyu, Du, Chao, Yang, Xiao, Li, Chongxuan, Cheung, Ngai-Man, Lin, Min.* arXiv preprint arXiv:2305.16934, 2023. [[abs](https://arxiv.org/abs/2305.16934)]
26. **Otter: A multi-modal model with in-context instruction tuning** *Li, Bo, Zhang, Yuanhan, Chen, Liangyu, Wang, Jinghao, Yang, Jingkang, Liu, Ziwei.* arXiv preprint arXiv:2305.03726, 2023. [[abs](https://arxiv.org/abs/2305.03726)]
27. **Minigpt-4: Enhancing vision-language understanding with advanced large language models** *Zhu, Deyao, Chen, Jun, Shen, Xiaoqian, Li, Xiang, Elhoseiny, Mohamed.* arXiv preprint arXiv:2304.10592, 2023. [[abs](https://arxiv.org/abs/2304.10592)]
28. **Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models** *Li, Junnan, Li, Dongxu, Savarese, Silvio, Hoi, Steven.* No journal, 2023.
29. **Visionllm: Large language model is also an open-ended decoder for vision-centric tasks** *Wang, Wenhai, Chen, Zhe, Chen, Xiaokang, Wu, Jiannan, Zhu, Xizhou, Zeng, Gang, Luo, Ping, Lu, Tong, Zhou, Jie, Qiao, Yu, others.* arXiv preprint arXiv:2305.11175, 2023. [[abs](https://arxiv.org/abs/2305.11175)]
30. **Grounding language models to images for multimodal inputs and outputs** *Koh, Jing Yu, Salakhutdinov, Ruslan, Fried, Daniel.* Presented at International Conference on Machine Learning, 2023. [Link](No URL)
31. **3D-GPT: Procedural 3D Modeling with Large Language Models** *Sun, Chunyi, Han, Junlin, Deng, Weijian, Wang, Xinlong, Qin, Zishan, Gould, Stephen.* arXiv preprint arXiv:2310.12945, 2023. [[abs](https://arxiv.org/abs/2310.12945)]
32. **Llama-adapter: Efficient fine-tuning of language models with zero-init attention** *Zhang, Renrui, Han, Jiaming, Zhou, Aojun, Hu, Xiangfei, Yan, Shilin, Lu, Pan, Li, Hongsheng, Gao, Peng, Qiao, Yu.* arXiv preprint arXiv:2303.16199, 2023. [[abs](https://arxiv.org/abs/2303.16199)]
33. **Visual instruction tuning** *Liu, Haotian, Li, Chunyuan, Wu, Qingyang, Lee, Yong Jae.* arXiv preprint arXiv:2304.08485, 2023. [[abs](https://arxiv.org/abs/2304.08485)]
34. **Multimodal-gpt: A vision and language model for dialogue with humans** *Gong, Tao, Lyu, Chengqi, Zhang, Shilong, Wang, Yudong, Zheng, Miao, Zhao, Qian, Liu, Kuikun, Zhang, Wenwei, Luo, Ping, Chen, Kai.* arXiv preprint arXiv:2305.04790, 2023. [[abs](https://arxiv.org/abs/2305.04790)]
35. **Towards language models that can see: Computer vision through the lens of natural language** *Berrios, William, Mittal, Gautam, Thrush, Tristan, Kiela, Douwe, Singh, Amanpreet.* arXiv preprint arXiv:2306.16410, 2023. [[abs](https://arxiv.org/abs/2306.16410)]
36. **Llama-adapter v2: Parameter-efficient visual instruction model** *Gao, Peng, Han, Jiaming, Zhang, Renrui, Lin, Ziyi, Geng, Shijie, Zhou, Aojun, Zhang, Wei, Lu, Pan, He, Conghui, Yue, Xiangyu, others.* arXiv preprint arXiv:2304.15010, 2023. [[abs](https://arxiv.org/abs/2304.15010)]
37. **Multimodal large language models: A survey** *Wu, Jiayang, Gan, Wensheng, Chen, Zefeng, Wan, Shicheng, Philip, S Yu.* Presented at 2023 IEEE International Conference on Big Data (BigData), 2023. [Link](No URL)
38. **Visual adversarial examples jailbreak aligned large language models** *Qi, Xiangyu, Huang, Kaixuan, Panda, Ashwinee, Wang, Mengdi, Mittal, Prateek.* Presented at The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023. [Link](No URL)
39. **Introducing our Multimodal Models** *Bavishi, Rohan, Elsen, Erich, Hawthorne, Curtis, Nye, Maxwell, Odena, Augustus, Somani, Arushi,  Ta\cs.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
40. **Gemini: a family of highly capable multimodal models** *Team, Gemini, Anil, Rohan, Borgeaud, Sebastian, Wu, Yonghui, Alayrac, Jean-Baptiste, Yu, Jiahui, Soricut, Radu, Schalkwyk, Johan, Dai, Andrew M, Hauth, Anja, others.* arXiv preprint arXiv:2312.11805, 2023. [[abs](https://arxiv.org/abs/2312.11805)]
41. **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic** *Chen, Keqin, Zhang, Zhao, Zeng, Weili, Zhang, Richong, Zhu, Feng, Zhao, Rui.* arXiv preprint arXiv:2306.15195, 2023. [[abs](https://arxiv.org/abs/2306.15195)]
42. **OtterHD: A High-Resolution Multi-modality Model** *Li, Bo, Zhang, Peiyuan, Yang, Jingkang, Zhang, Yuanhan, Pu, Fanyi, Liu, Ziwei.* arXiv preprint arXiv:2311.04219, 2023. [[abs](https://arxiv.org/abs/2311.04219)]
43. **Minigpt-5: Interleaved vision-and-language generation via generative vokens** *Zheng, Kaizhi, He, Xuehai, Wang, Xin Eric.* arXiv preprint arXiv:2310.02239, 2023. [[abs](https://arxiv.org/abs/2310.02239)]
44. **A Simple LLM Framework for Long-Range Video Question-Answering** *Zhang, Ce, Lu, Taixi, Islam, Md Mohaiminul, Wang, Ziyang, Yu, Shoubin, Bansal, Mohit, Bertasius, Gedas.* arXiv preprint arXiv:2312.17235, 2023. [[abs](https://arxiv.org/abs/2312.17235)]
45. **Camel: Communicative agents for" mind" exploration of large scale language model society** *Li, Guohao, Hammoud, Hasan Abed Al Kader, Itani, Hani, Khizbullin, Dmitrii, Ghanem, Bernard.* Presented at No conference, 2023. [Link](No URL)
46. **Video-llama: An instruction-tuned audio-visual language model for video understanding** *Zhang, Hang, Li, Xin, Bing, Lidong.* arXiv preprint arXiv:2306.02858, 2023. [[abs](https://arxiv.org/abs/2306.02858)]
47. **Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning** *Zhao, Bingchen, Tu, Haoqin, Wei, Chen, Mei, Jieru, Xie, Cihang.* arXiv preprint arXiv:2312.11420, 2023. [[abs](https://arxiv.org/abs/2312.11420)]
48. **SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models** *Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Jiao Qiao.* ArXiv, 2023.
49. **Gpt4tools: Teaching large language model to use tools via self-instruction** *Yang, Rui, Song, Lin, Li, Yanwei, Zhao, Sijie, Ge, Yixiao, Li, Xiu, Shan, Ying.* arXiv preprint arXiv:2305.18752, 2023. [[abs](https://arxiv.org/abs/2305.18752)]
50. **Generative pretraining in multimodality** *Sun, Quan, Yu, Qiying, Cui, Yufeng, Zhang, Fan, Zhang, Xiaosong, Wang, Yueze, Gao, Hongcheng, Liu, Jingjing, Huang, Tiejun, Wang, Xinlong.* arXiv preprint arXiv:2307.05222, 2023. [[abs](https://arxiv.org/abs/2307.05222)]
51. **Agents: An open-source framework for autonomous language agents** *Zhou, Wangchunshu, Jiang, Yuchen Eleanor, Li, Long, Wu, Jialong, Wang, Tiannan, Qiu, Shi, Zhang, Jintian, Chen, Jing, Wu, Ruipu, Wang, Shuai, others.* arXiv preprint arXiv:2309.07870, 2023. [[abs](https://arxiv.org/abs/2309.07870)]
52. **LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment** *Zhu, Bin, Lin, Bin, Ning, Munan, Yan, Yang, Cui, Jiaxi, Wang, HongFa, Pang, Yatian, Jiang, Wenhao, Zhang, Junwu, Li, Zongwei, others.* arXiv preprint arXiv:2310.01852, 2023. [[abs](https://arxiv.org/abs/2310.01852)]
53. **Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics** *Tu, Haoqin, Zhao, Bingchen, Wei, Chen, Xie, Cihang.* Presented at NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [Link](No URL)
54. **How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs** *Tu, Haoqin, Cui, Chenhang, Wang, Zijun, Zhou, Yiyang, Zhao, Bingchen, Han, Junlin, Zhou, Wangchunshu, Yao, Huaxiu, Xie, Cihang.* arXiv preprint arXiv:2311.16101, 2023. [[abs](https://arxiv.org/abs/2311.16101)]
55. **Imagebind: One embedding space to bind them all** *Girdhar, Rohit, El-Nouby, Alaaeldin, Liu, Zhuang, Singh, Mannat, Alwala, Kalyan Vasudev, Joulin, Armand, Misra, Ishan.* Presented at Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [Link](No URL)
56. **Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices** *Chu, Xiangxiang, Qiao, Limeng, Lin, Xinyang, Xu, Shuang, Yang, Yang, Hu, Yiming, Wei, Fei, Zhang, Xinyu, Zhang, Bo, Wei, Xiaolin, others.* arXiv preprint arXiv:2312.16886, 2023. [[abs](https://arxiv.org/abs/2312.16886)]
57. **Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing** *Chen, Wei-Ge, Spiridonova, Irina, Yang, Jianwei, Gao, Jianfeng, Li, Chunyuan.* arXiv preprint arXiv:2311.00571, 2023. [[abs](https://arxiv.org/abs/2311.00571)]
58. **What Makes for Good Visual Tokenizers for Large Language Models?** *Wang, Guangzhi, Ge, Yixiao, Ding, Xiaohan, Kankanhalli, Mohan, Shan, Ying.* arXiv preprint arXiv:2305.12223, 2023. [[abs](https://arxiv.org/abs/2305.12223)]
59. **Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi** *Yue, Xiang, Ni, Yuansheng, Zhang, Kai, Zheng, Tianyu, Liu, Ruoqi, Zhang, Ge, Stevens, Samuel, Jiang, Dongfu, Ren, Weiming, Sun, Yuxuan, others.* arXiv preprint arXiv:2311.16502, 2023. [[abs](https://arxiv.org/abs/2311.16502)]
60. **Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences** *Wang, Xiyao, Zhou, Yuhang, Liu, Xiaoyu, Lu, Hongjin, Xu, Yuancheng, He, Feihong, Yoon, Jaehong, Lu, Taixi, Bertasius, Gedas, Bansal, Mohit, others.* arXiv preprint arXiv:2401.10529, 2024. [[abs](https://arxiv.org/abs/2401.10529)]
61. **Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA** *Fan, Yue, Gu, Jing, Zhou, Kaiwen, Yan, Qianqi, Jiang, Shan, Kuo, Ching-Chen, Guan, Xinze, Wang, Xin Eric.* arXiv preprint arXiv:2401.15847, 2024. [[abs](https://arxiv.org/abs/2401.15847)]
62. **Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs** *Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie.* ArXiv, 2024.
63. **LEGO: Language Enhanced Multi-modal Grounding Model** *Li, Zhaowei, Xu, Qi, Zhang, Dong, Song, Hang, Cai, Yiqing, Qi, Qi, Zhou, Ran, Pan, Junting, Li, Zefeng, Vu, Van Tu, others.* arXiv preprint arXiv:2401.06071, 2024. [[abs](https://arxiv.org/abs/2401.06071)]
64. **Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models** *He, Xin, Wei, Longhui, Xie, Lingxi, Tian, Qi.* arXiv preprint arXiv:2401.03105, 2024. [[abs](https://arxiv.org/abs/2401.03105)]

### 2.2 Reasoning
1. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**. *Jason Wei* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2201.11903)]
4. **Decomposed Prompting: A Modular Approach for Solving Complex Tasks**. *Tushar Khot* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2210.02406)]
5. **Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs**. *Maarten Sap* et al. EMNLP 2022. [[paper](https://arxiv.org/abs/2210.13312)]
6. **Inner Monologue: Embodied Reasoning through Planning with Language Models**. *Wenlong Huang* et al. CoRL 2022. [[paper](https://arxiv.org/abs/2207.05608)]
7. **Survey of Hallucination in Natural Language Generation**. *Ziwei Ji* et al. ACM Computing Surveys 2022. [[paper](https://arxiv.org/abs/2202.03629)]
8. **ReAct: Synergizing Reasoning and Acting in Language Models**. *Shunyu Yao* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.03629)]
3. **Complexity-Based Prompting for Multi-Step Reasoning**. *Yao Fu* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.00720)]
7. **Towards Reasoning in Large Language Models: A Survey**. *Jie Huang* et al. ACL Findings 2023. [[paper](https://arxiv.org/abs/2212.10403)]
9. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**. *Denny Zhou* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2205.10625)]
10. **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models**. *Ishika Singh* et al. ICRA 2023. [[paper](https://arxiv.org/abs/2209.11302)]
11. **Reasoning with Language Model is Planning with World Model**. *Shibo Hao* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.14992)]
12. **Evaluating Object Hallucination in Large Vision-Language Models**. *Yifan Li* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.10355)]
13. **Tree of Thoughts: Deliberate Problem Solving with Large Language Models**. *Shunyu Yao* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2305.10601)]
14. **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**. *Zihao Wang* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2302.01560)]
15. **LLM+P: Empowering Large Language Models with Optimal Planning Proficiency**. *Bo Liu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2304.11477)]
16. **Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning**. *Zhiting Hu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.05230)]
17. **MMToM-QA: Multimodal Theory of Mind Question Answering**. *Chuanyang Jin* et al. arXiv 2024. [[paper](https://arxiv.org/abs/2401.08743)]
18. **Graph of Thoughts: Solving Elaborate Problems with Large Language Models**. *Maciej Besta* et al. AAAI 2024. [[paper](https://arxiv.org/abs/2308.09687)]

### 2.3 Memory
1. **Dense Passage Retrieval for Open-Domain Question Answering**. *Vladimir Karpukhin* et al. EMNLP 2020. [[paper](https://arxiv.org/abs/2004.04906)]
2. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**. *Patrick Lewis* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2005.11401)]
3. **REALM: Retrieval-Augmented Language Model Pre-Training**. *Kelvin Guu* et al. ICML 2020. [[paper](https://arxiv.org/abs/2002.08909)]
4. **Retrieval Augmentation Reduces Hallucination in Conversation**. *Kurt Shuster* et al. EMNLP Findings 2021. [[paper](https://arxiv.org/abs/2104.07567)]
5. **Improving Language Models by Retrieving from Trillions of Tokens**. *Sebastian Borgeaud* et al. ICML 2022. [[paper](https://arxiv.org/abs/2112.04426)]
6. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**. *Tri Dao* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2205.14135)]
7. **Generative Agents: Interactive Simulacra of Human Behavior**. *Joon Sung Park* et al. UIST 2023. [[paper](https://arxiv.org/abs/2304.03442)]
8. **Cognitive Architectures for Language Agents**. *Theodore R. Sumers* et al. TMLR 2024. [[paper](https://arxiv.org/abs/2309.02427)]

### 2.4 Metacognition
1. **Evolving Self-supervised Neural Networks: Autonomous Intelligence from Evolved Self-teaching**
   *Nam Le.* arXiv, 2019. [\[eprint\]](https://arxiv.org/abs/1906.08865)

2. **Self-Instruct: Aligning Language Models with Self-Generated Instructions**
   *Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi.* arXiv, 2022. [\[eprint\]](https://arxiv.org/abs/2212.10560)

3. **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**
   *Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar.* arXiv, 2023. [\[eprint\]](https://arxiv.org/abs/2312.10003)

4. **Wizardlm: Empowering large language models to follow complex instructions**
   *Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang.* arXiv, 2023. [\[abs\]](https://arxiv.org/abs/2304.12244)

## 3. AGI Interface: Connecting the World with AGI
### 3.1 Interfaces to Digital World
### 3.2 Interfaces to Physical World
### 3.3 Interfaces to Intelligence

#### 3.3.1 Interfaces to AI Agents

1. **A general language assistant as a laboratory for alignment** *Askell, Amanda, Bai, Yuntao, Chen, Anna, Drain, Dawn, Ganguli, Deep, Henighan, Tom, Jones, Andy, Joseph, Nicholas, Mann, Ben, DasSarma, Nova, others.* arXiv preprint arXiv:2112.00861, 2021. [[abs](https://arxiv.org/abs/2112.00861)]
2. **Glam: Efficient scaling of language models with mixture-of-experts** *Du, Nan, Huang, Yanping, Dai, Andrew M, Tong, Simon, Lepikhin, Dmitry, Xu, Yuanzhong, Krikun, Maxim, Zhou, Yanqi, Yu, Adams Wei, Firat, Orhan, others.* Presented at International Conference on Machine Learning, 2022. [Link](No URL)
3. **In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models** *Huang, Yukun, Chen, Yanda, Yu, Zhou, McKeown, Kathleen.* arXiv preprint arXiv:2212.10670, 2022. [[abs](https://arxiv.org/abs/2212.10670)]
4. **Explanations from large language models make small reasoners better** *Li, Shiyang, Chen, Jianshu, Shen, Yelong, Chen, Zhiyu, Zhang, Xinlu, Li, Zekun, Wang, Hong, Qian, Jing, Peng, Baolin, Mao, Yi, others.* arXiv preprint arXiv:2210.06726, 2022. [[abs](https://arxiv.org/abs/2210.06726)]
5. **Tailoring self-rationalizers with multi-reward distillation** *Ramnath, Sahana, Joshi, Brihi, Hallinan, Skyler, Lu, Ximing, Li, Liunian Harold, Chan, Aaron, Hessel, Jack, Choi, Yejin, Ren, Xiang.* arXiv preprint arXiv:2311.02805, 2023. [[abs](https://arxiv.org/abs/2311.02805)]
6. **OpenMoE: Open Mixture-of-Experts Language Models** *Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
7. **Knowledge Distillation of Large Language Models** *Yuxian Gu, Li Dong, Furu Wei, Minlie Huang.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.08543)]
8. **Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes** *Hsieh, Cheng-Yu, Li, Chun-Liang, Yeh, Chih-Kuan, Nakhost, Hootan, Fujii, Yasuhisa, Ratner, Alexander, Krishna, Ranjay, Lee, Chen-Yu, Pfister, Tomas.* arXiv preprint arXiv:2305.02301, 2023. [[abs](https://arxiv.org/abs/2305.02301)]
9. **Camel: Communicative agents for" mind" exploration of large scale language model society** *Li, Guohao, Hammoud, Hasan Abed Al Kader, Itani, Hani, Khizbullin, Dmitrii, Ghanem, Bernard.* arXiv preprint arXiv:2303.17760, 2023. [[abs](https://arxiv.org/abs/2303.17760)]
10. **Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents** *Chen, Weize, Su, Yusheng, Zuo, Jingwei, Yang, Cheng, Yuan, Chenfei, Qian, Chen, Chan, Chi-Min, Qin, Yujia, Lu, Yaxi, Xie, Ruobing, others.* arXiv preprint arXiv:2308.10848, 2023. [[abs](https://arxiv.org/abs/2308.10848)]
11. **Phi-2: The surprising power of small language models** *Javaheripi, Mojan, Bubeck, S\'e.* Microsoft Research Blog, 2023.
12. **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning** *Yuchen Lin, Bill, Ravichander, Abhilasha, Lu, Ximing, Dziri, Nouha, Sclar, Melanie, Chandu, Khyathi, Bhagavatula, Chandra, Choi, Yejin.* arXiv preprint arXiv:arXiv e-prints, 2023. [[abs](https://arxiv.org/abs/arXiv e-prints)]
13. **LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents** *Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li.* arXiv preprint arXiv:2311.05437, 2023. [[abs](https://arxiv.org/abs/2311.05437)]
14. **Neural amortized inference for nested multi-agent reasoning** *Jha, Kunal, Le, Tuan Anh, Jin, Chuanyang, Kuo, Yen-Ling, Tenenbaum, Joshua B, Shu, Tianmin.* arXiv preprint arXiv:2308.11071, 2023. [[abs](https://arxiv.org/abs/2308.11071)]
15. **Textbooks are all you need ii: phi-1.5 technical report** *Li, Yuanzhi, Bubeck, S\'e.* arXiv preprint arXiv:2309.05463, 2023. [[abs](https://arxiv.org/abs/2309.05463)]
16. **Lion: Adversarial Distillation of Closed-Source Large Language Model** *Jiang, Yuxin, Chan, Chunkit, Chen, Mingyang, Wang, Wei.* arXiv preprint arXiv:2305.12870, 2023. [[abs](https://arxiv.org/abs/2305.12870)]
17. **Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks** *Chowdhury, Mohammed Nowaz Rabbani, Zhang, Shuai, Wang, Meng, Liu, Sijia, Chen, Pin-Yu.* arXiv preprint arXiv:2306.04073, 2023. [[abs](https://arxiv.org/abs/2306.04073)]
18. **Symbolic chain-of-thought distillation: Small models can also" think" step-by-step** *Li, Liunian Harold, Hessel, Jack, Yu, Youngjae, Ren, Xiang, Chang, Kai-Wei, Choi, Yejin.* arXiv preprint arXiv:2306.14050, 2023. [[abs](https://arxiv.org/abs/2306.14050)]
19. **ChatGPT outperforms crowd workers for text-annotation tasks** *Gilardi, Fabrizio, Alizadeh, Meysam, Kubli, Ma\"e.* Proceedings of the National Academy of Sciences, 2023. [[abs](https://arxiv.org/abs/2303.15056)]
20. **Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial Intelligence (AI), and higher education: A systematic review** *Memarian, Bahar, Doleck, Tenzin.* Computers and Education: Artificial Intelligence, 2023. [Link](https://www.sciencedirect.com/science/article/pii/S2666920X23000310)
21. **Metagpt: Meta programming for multi-agent collaborative framework** *Hong, Sirui, Zheng, Xiawu, Chen, Jonathan, Cheng, Yuheng, Wang, Jinlin, Zhang, Ceyao, Wang, Zili, Yau, Steven Ka Shing, Lin, Zijuan, Zhou, Liyang, others.* arXiv preprint arXiv:2308.00352, 2023. [[abs](https://arxiv.org/abs/2308.00352)]
22. **Autoagents: A framework for automatic agent generation** *Chen, Guangyao, Dong, Siwei, Shu, Yu, Zhang, Ge, Sesay, Jaward, Karlsson, B\"o.* arXiv preprint arXiv:2309.17288, 2023. [[abs](https://arxiv.org/abs/2309.17288)]
23. **Specializing Smaller Language Models towards Multi-Step Reasoning** *Fu, Yao, Peng, Hao, Ou, Litu, Sabharwal, Ashish, Khot, Tushar.* arXiv preprint arXiv:2301.12726, 2023. [[abs](https://arxiv.org/abs/2301.12726)]
24. **Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision** *Burns, Collin, Izmailov, Pavel, Kirchner, Jan Hendrik, Baker, Bowen, Gao, Leo, Aschenbrenner, Leopold, Chen, Yining, Ecoffet, Adrien, Joglekar, Manas, Leike, Jan, others.* arXiv preprint arXiv:2312.09390, 2023. [[abs](https://arxiv.org/abs/2312.09390)]
25. **Mindstorms in natural language-based societies of mind** *Zhuge, Mingchen, Liu, Haozhe, Faccio, Francesco, Ashley, Dylan R, Csord\'a.* arXiv preprint arXiv:2305.17066, 2023. [[abs](https://arxiv.org/abs/2305.17066)]
26. **Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization** *Liu, Zijun, Zhang, Yanzhe, Li, Peng, Liu, Yang, Yang, Diyi.* arXiv preprint arXiv:2310.02170, 2023. [[abs](https://arxiv.org/abs/2310.02170)]
27. **Agents: An open-source framework for autonomous language agents** *Zhou, Wangchunshu, Jiang, Yuchen Eleanor, Li, Long, Wu, Jialong, Wang, Tiannan, Qiu, Shi, Zhang, Jintian, Chen, Jing, Wu, Ruipu, Wang, Shuai, others.* arXiv preprint arXiv:2309.07870, 2023. [[abs](https://arxiv.org/abs/2309.07870)]
28. **Stanford Alpaca: An Instruction-following LLaMA model** *Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
29. **Octavius: Mitigating Task Interference in MLLMs via MoE** *Chen, Zeren, Wang, Ziqin, Wang, Zhen, Liu, Huayang, Yin, Zhenfei, Liu, Si, Sheng, Lu, Ouyang, Wanli, Qiao, Yu, Shao, Jing.* arXiv preprint arXiv:2311.02684, 2023. [[abs](https://arxiv.org/abs/2311.02684)]
30. **Enhancing chat language models by scaling high-quality instructional conversations** *Ding, Ning, Chen, Yulin, Xu, Bokai, Qin, Yujia, Zheng, Zhi, Hu, Shengding, Liu, Zhiyuan, Sun, Maosong, Zhou, Bowen.* arXiv preprint arXiv:2305.14233, 2023. [[abs](https://arxiv.org/abs/2305.14233)]
31. **Communicative agents for software development** *Qian, Chen, Cong, Xin, Yang, Cheng, Chen, Weize, Su, Yusheng, Xu, Juyuan, Liu, Zhiyuan, Sun, Maosong.* arXiv preprint arXiv:2307.07924, 2023. [[abs](https://arxiv.org/abs/2307.07924)]
32. **Principle-driven self-alignment of language models from scratch with minimal human supervision** *Sun, Zhiqing, Shen, Yikang, Zhou, Qinhong, Zhang, Hongxin, Chen, Zhenfang, Cox, David, Yang, Yiming, Gan, Chuang.* Advances in Neural Information Processing Systems, 2024. [[abs](https://arxiv.org/abs/2305.03047)]
33. **Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models** *Guo, Jianyuan, Chen, Hanting, Wang, Chengcheng, Han, Kai, Xu, Chang, Wang, Yunhe.* arXiv preprint arXiv:2402.03749, 2024. [[abs](https://arxiv.org/abs/2402.03749)]
34. **Aligner: Achieving efficient alignment through weak-to-strong correction** *Ji, Jiaming, Chen, Boyuan, Lou, Hantao, Hong, Donghai, Zhang, Borong, Pan, Xuehai, Dai, Juntao, Yang, Yaodong.* arXiv preprint arXiv:2402.02416, 2024. [[abs](https://arxiv.org/abs/2402.02416)]
35. **Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision** *Sun, Zhiqing, Yu, Longhui, Shen, Yikang, Liu, Weiyang, Yang, Yiming, Welleck, Sean, Gan, Chuang.* arXiv preprint arXiv:2403.09472, 2024. [[abs](https://arxiv.org/abs/2403.09472)]
36. **Self-play fine-tuning converts weak language models to strong language models** *Chen, Zixiang, Deng, Yihe, Yuan, Huizhuo, Ji, Kaixuan, Gu, Quanquan.* arXiv preprint arXiv:2401.01335, 2024. [[abs](https://arxiv.org/abs/2401.01335)]

#### 3.3.2 Interfaces to Humans

1. **Guidelines for Human-AI Interaction**
   *Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, Eric Horvitz*. CHI 2019. [[paper](https://dl.acm.org/doi/10.1145/3290605.3300233)]
2. **Design Principles for Generative AI Applications**
   *Justin D. Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, Werner Geyer*. CHI 2024. [[paper](http://arxiv.org/abs/2401.14484)]
3. **Graphologue: Exploring Large Language Model Responses with Interactive Diagrams**
   *Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606737)]
4. **Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models**
   *Sangho Suh, Bryan Min, Srishti Palani, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606756)]
5. **Supporting Sensemaking of Large Language Model Outputs at Scale**
   *Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman*. CHI 2024. [[paper](https://arxiv.org/abs/2401.13726)]
6. **Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation**
   *Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia*. CHI 2024. [[Paper](http://arxiv.org/abs/2310.12953)]
7. **AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts**
   *Tongshuang Wu, Michael Terry, Carrie Jun Cai*. CHI 2022. [[paper](https://dl.acm.org/doi/10.1145/3491102.3517582)]
8. **Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**
   *Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606725)]
9. **ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing**
   *Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman*. CHI 2024. [[paper](https://doi.org/10.48550/arXiv.2309.09128)]
10. **CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming**
    *Li Feng, Ryan Yen, Yuzhe You, Mingming Fan, Jian Zhao, Zhicong Lu*. CHI 2024. [[paper](http://arxiv.org/abs/2310.09235)]
11. **Generating Automatic Feedback on UI Mockups with Large Language Models**
    *Peitong Duan, Jeremy Warner, Yang Li, Björn Hartmann*. CHI 2024. [[paper](http://arxiv.org/abs/2403.13139)]
12. **Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation**
    *Susan Lin, Jeremy Warner, J. D. Zamfirescu-Pereira, Matthew G. Lee, Sauhard Jain, Michael Xuelin Huang, Piyawat Lertvittayakumjorn, Shanqing Cai, Shumin Zhai, Björn Hartmann, Can Liu*. CHI 2024. [[paper](http://arxiv.org/abs/2401.10838)]
13. **Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy**
    *Efe Bozkir, Süleyman Özdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci*. arXiv 2024. [[paper](http://arxiv.org/abs/2402.03907)]
14. **GenAssist: Making Image Generation Accessible**
    *Mina Huh, Yi-Hao Peng, Amy Pavel*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606735)]
15. **“The less I type, the better”: How AI Language Models can Enhance or Impede Communication for AAC Users**
    *Stephanie Valencia, Richard Cave, Krystal Kallarackal, Katie Seaver, Michael Terry, Shaun K. Kane*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581560)]
16. **Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design**
    *Qian Yang, Aaron Steinfeld, Carolyn Rosé, John Zimmerman*. CHI 2020. [[paper](https://dl.acm.org/doi/10.1145/3313831.3376301)]

## 4. AGI Systems: Implementing the Mechanism of AGI
### 4.1 System Challenges
### 4.2 Scalable Model Architectures
### 4.3 Large-scale Training
### 4.4 Inference Techniques
### 4.5 Cost and Efficiency
### 4.6 Computing Platforms
### 4.7 The Future of AGI Systems

## 5. AGI Alignment: Reconciling Needs with AGI
### 5.1 Expectations of AGI Alignment

1. **Human Compatible: Artificial Intelligence and the Problem of Control**
   *Stuart Russell*. Viking, 2019.
2. **Artificial Intelligence, Values and Alignment**
   *Iason Gabriel*. Minds and Machines, 2020. [[paper](http://arxiv.org/abs/2001.09768)]
3. **Alignment of Language Agents**
   *Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving*. arXiv, 2021. [[Paper](http://arxiv.org/abs/2103.14659)]
4. **The Value Learning Problem** 
   *Nate Soares*. Machine Intelligence Research Institute Technical Report [[paper](https://intelligence.org/files/ValueLearningProblem.pdf)]
5. **Concrete Problems in AI Safety**
   *Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané*. arXiv, 2016. [[paper](http://arxiv.org/abs/1606.06565)]
6. **Ethical and social risks of harm from Language Models**
   *Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel*. arXiv, 2021. [[paper](http://arxiv.org/abs/2112.04359)]
7. **On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?**
   *Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell*. FAccT 2021. [[paper](https://dl.acm.org/doi/10.1145/3442188.3445922)]
8. **The global landscape of AI ethics guidelines**
   *Anna Jobin, Marcello Ienca, Effy Vayena*. Nature Machine Intelligence, 2019. [[paper](https://www.nature.com/articles/s42256-019-0088-2)]
9. **Persistent Anti-Muslim Bias in Large Language Models**
   *Abubakar Abid, Maheen Farooqi, James Zou*. AIES, 2021. [[paper](http://arxiv.org/abs/2101.05783)]
10. **Toward Gender-Inclusive Coreference Resolution**
      *Yang Trista Cao, Hal Daumé III*. ACL, 2020. [[paper](https://aclanthology.org/2020.acl-main.418)]
11. **The Social Impact of Natural Language Processing**
    *Dirk Hovy, Shannon L. Spruit*. ACL 2016. [[paper](https://aclanthology.org/P16-2096)]
12. **TruthfulQA: Measuring How Models Mimic Human Falsehoods**
    *Stephanie Lin, Jacob Hilton, Owain Evans*. ACL 2022. [[paper](https://aclanthology.org/2022.acl-long.229)]
13. **The Radicalization Risks of GPT-3 and Advanced Neural Language Models**
    *Kris McGuffie, Alex Newhouse*. arXiv, 2020. [[paper](http://arxiv.org/abs/2009.06807)]
14. **AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap**
    *Q. Vera Liao, Jennifer Wortman Vaughan*. arXiv 2023. [[paper](http://arxiv.org/abs/2306.01941)]
15. **Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs**
    *Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan*. CHI 2021. [[paper](https://dl.acm.org/doi/10.1145/3411764.3445088)]
16. **Identifying and Mitigating the Security Risks of Generative AI**
    *Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang*. arXiv, 2023. [[paper](http://arxiv.org/abs/2308.14840)]
17. **LLM Agents can Autonomously Hack Websites**
    *Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang*. arXiv, 2024. [[paper](http://arxiv.org/abs/2402.06664)]
18. **Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks**
    *Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi, Sauvik Das*. CHI 2024. [[paper](http://arxiv.org/abs/2310.07879)]
19. **Privacy in the Age of AI**
    *Sauvik Das, Hao-Ping (Hank) Lee, Jodi Forlizzi*. Communications of the ACM, 2023. [[paper](https://dl.acm.org/doi/10.1145/3625254)]

### 5.2 Current Alignment Techniques

1. **AI safety via debate** *Irving, Geoffrey, Christiano, Paul, Amodei, Dario.* arXiv preprint arXiv:1805.00899, 2018. [[abs](https://arxiv.org/abs/1805.00899)]
2. **AI safety needs social scientists** *Irving, Geoffrey, Askell, Amanda.* Distill, 2019. [Link](https://distill.pub/2019/safety-needs-social-scientists/)
3. **Learning to summarize with human feedback** *Stiennon, Nisan, Ouyang, Long, Wu, Jeffrey, Ziegler, Daniel, Lowe, Ryan, Voss, Chelsea, Radford, Alec, Amodei, Dario, Christiano, Paul F.* Advances in Neural Information Processing Systems, 2020. [[abs](https://arxiv.org/abs/2009.01325)]
4. **Slic-hf: Sequence likelihood calibration with human feedback** *Zhao, Yao, Joshi, Rishabh, Liu, Tianqi, Khalman, Misha, Saleh, Mohammad, Liu, Peter J.* arXiv preprint arXiv:2305.10425, 2023. [[abs](https://arxiv.org/abs/2305.10425)]
5. **The cringe loss: Learning what language not to model** *Adolphs, Leonard, Gao, Tianyu, Xu, Jing, Shuster, Kurt, Sukhbaatar, Sainbayar, Weston, Jason.* arXiv preprint arXiv:2211.05826, 2022. [[abs](https://arxiv.org/abs/2211.05826)]
6. **Second thoughts are best: Learning to re-align with human values from text edits** *Liu, Ruibo, Jia, Chenyan, Zhang, Ge, Zhuang, Ziyu, Liu, Tony, Vosoughi, Soroush.* Advances in Neural Information Processing Systems, 2022. [[abs](https://arxiv.org/abs/2301.00355)]
7. **Training language models to follow instructions with human feedback** *Ouyang, Long, Wu, Jeffrey, Jiang, Xu, Almeida, Diogo, Wainwright, Carroll, Mishkin, Pamela, Zhang, Chong, Agarwal, Sandhini, Slama, Katarina, Ray, Alex, others.* Advances in neural information processing systems, 2022. [[abs](https://arxiv.org/abs/2203.02155)]
8. **Leashing the Inner Demons: Self-Detoxification for Language Models** *Xu, Canwen, He, Zexue, He, Zhankui, McAuley, Julian.* Presented at Proceedings of the AAAI Conference on Artificial Intelligence, 2022. [Link](No URL)
9. **Aligning generative language models with human values** *Liu, Ruibo, Zhang, Ge, Feng, Xinyu, Vosoughi, Soroush.* Presented at Findings of the Association for Computational Linguistics: NAACL 2022, 2022. [Link](No URL)
10. **Training a helpful and harmless assistant with reinforcement learning from human feedback** *Bai, Yuntao, Jones, Andy, Ndousse, Kamal, Askell, Amanda, Chen, Anna, DasSarma, Nova, Drain, Dawn, Fort, Stanislav, Ganguli, Deep, Henighan, Tom, others.* arXiv preprint arXiv:2204.05862, 2022. [[abs](https://arxiv.org/abs/2204.05862)]
11. **Constitutional ai: Harmlessness from ai feedback** *Bai, Yuntao, Kadavath, Saurav, Kundu, Sandipan, Askell, Amanda, Kernion, Jackson, Jones, Andy, Chen, Anna, Goldie, Anna, Mirhoseini, Azalia, McKinnon, Cameron, others.* arXiv preprint arXiv:2212.08073, 2022. [[abs](https://arxiv.org/abs/2212.08073)]
12. **Raft: Reward ranked finetuning for generative foundation model alignment** *Dong, Hanze, Xiong, Wei, Goyal, Deepanshu, Pan, Rui, Diao, Shizhe, Zhang, Jipeng, Shum, Kashun, Zhang, Tong.* arXiv preprint arXiv:2304.06767, 2023. [[abs](https://arxiv.org/abs/2304.06767)]
13. **Chain of hindsight aligns language models with feedback** *Liu, Hao, Sferrazza, Carmelo, Abbeel, Pieter.* arXiv preprint arXiv:2302.02676, 2023. [[abs](https://arxiv.org/abs/2302.02676)]
14. **Improving Language Models with Advantage-based Offline Policy Gradients** *Baheti, Ashutosh, Lu, Ximing, Brahman, Faeze, Bras, Ronan Le, Sap, Maarten, Riedl, Mark.* arXiv preprint arXiv:2305.14718, 2023. [[abs](https://arxiv.org/abs/2305.14718)]
15. **Training language models with language feedback at scale** *Scheurer, J\'e.* arXiv preprint arXiv:2303.16755, 2023. [[abs](https://arxiv.org/abs/2303.16755)]
16. **Slic-hf: Sequence likelihood calibration with human feedback** *Zhao, Yao, Joshi, Rishabh, Liu, Tianqi, Khalman, Misha, Saleh, Mohammad, Liu, Peter J.* arXiv preprint arXiv:2305.10425, 2023. [[abs](https://arxiv.org/abs/2305.10425)]
17. **Principle-driven self-alignment of language models from scratch with minimal human supervision** *Sun, Zhiqing, Shen, Yikang, Zhou, Qinhong, Zhang, Hongxin, Chen, Zhenfang, Cox, David, Yang, Yiming, Gan, Chuang.* arXiv preprint arXiv:2305.03047, 2023. [[abs](https://arxiv.org/abs/2305.03047)]
18. **RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs** *Aky\"u.* arXiv preprint arXiv:2305.08844, 2023. [[abs](https://arxiv.org/abs/2305.08844)]
19. **Aligning language models with preferences through f-divergence minimization** *Go, Dongyoung, Korbak, Tomasz, Kruszewski, Germ\'a.* arXiv preprint arXiv:2302.08215, 2023. [[abs](https://arxiv.org/abs/2302.08215)]
20. **A general theoretical paradigm to understand learning from human preferences** *Azar, Mohammad Gheshlaghi, Rowland, Mark, Piot, Bilal, Guo, Daniel, Calandriello, Daniele, Valko, Michal, Munos, R\'e.* arXiv preprint arXiv:2310.12036, 2023. [[abs](https://arxiv.org/abs/2310.12036)]
21. **Let's Verify Step by Step** *Lightman, Hunter, Kosaraju, Vineet, Burda, Yura, Edwards, Harri, Baker, Bowen, Lee, Teddy, Leike, Jan, Schulman, John, Sutskever, Ilya, Cobbe, Karl.* arXiv preprint arXiv:2305.20050, 2023. [[abs](https://arxiv.org/abs/2305.20050)]
22. **AI safety via market making.** *Evan Hubinger.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
23. **Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons** *Zhu, Banghua, Jiao, Jiantao, Jordan, Michael I.* arXiv preprint arXiv:2301.11270, 2023. [[abs](https://arxiv.org/abs/2301.11270)]
24. **Open problems and fundamental limitations of reinforcement learning from human feedback** *Casper, Stephen, Davies, Xander, Shi, Claudia, Gilbert, Thomas Krendl, Scheurer, J\'e.* arXiv preprint arXiv:2307.15217, 2023. [[abs](https://arxiv.org/abs/2307.15217)]
25. **Aligning Large Language Models through Synthetic Feedback** *Kim, Sungdong, Bae, Sanghwan, Shin, Jamin, Kang, Soyoung, Kwak, Donghyun, Yoo, Kang Min, Seo, Minjoon.* arXiv preprint arXiv:2305.13735, 2023. [[abs](https://arxiv.org/abs/2305.13735)]
26. **Decentralized Training of Foundation Models in Heterogeneous Environments** *Binhang Yuan, Yongjun He, Jared Quincy Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Re, Ce Zhang.* arXiv 2023. [[abs](https://arxiv.org/abs/2206.01288)]
27. **Rlaif: Scaling reinforcement learning from human feedback with ai feedback** *Lee, Harrison, Phatale, Samrat, Mansoor, Hassan, Lu, Kellie, Mesnard, Thomas, Bishop, Colton, Carbune, Victor, Rastogi, Abhinav.* arXiv preprint arXiv:2309.00267, 2023. [[abs](https://arxiv.org/abs/2309.00267)]
28. **Preference ranking optimization for human alignment** *Song, Feifan, Yu, Bowen, Li, Minghao, Yu, Haiyang, Huang, Fei, Li, Yongbin, Wang, Houfeng.* arXiv preprint arXiv:2306.17492, 2023. [[abs](https://arxiv.org/abs/2306.17492)]
29. **Improving Factuality and Reasoning in Language Models through Multiagent Debate** *Du, Yilun, Li, Shuang, Torralba, Antonio, Tenenbaum, Joshua B, Mordatch, Igor.* arXiv preprint arXiv:2305.14325, 2023. [[abs](https://arxiv.org/abs/2305.14325)]
30. **Large language model alignment: A survey** *Shen, Tianhao, Jin, Renren, Huang, Yufei, Liu, Chuang, Dong, Weilong, Guo, Zishan, Wu, Xinwei, Liu, Yan, Xiong, Deyi.* arXiv preprint arXiv:2309.15025, 2023. [[abs](https://arxiv.org/abs/2309.15025)]
31. **Direct preference optimization: Your language model is secretly a reward model** *Rafailov, Rafael, Sharma, Archit, Mitchell, Eric, Manning, Christopher D, Ermon, Stefano, Finn, Chelsea.* Advances in Neural Information Processing Systems, 2024.
32. **Self-play fine-tuning converts weak language models to strong language models** *Chen, Zixiang, Deng, Yihe, Yuan, Huizhuo, Ji, Kaixuan, Gu, Quanquan.* arXiv preprint arXiv:2401.01335, 2024. [[abs](https://arxiv.org/abs/2401.01335)]
33. **Guiding large language models via directional stimulus prompting** *Li, Zekun, Peng, Baolin, He, Pengcheng, Galley, Michel, Gao, Jianfeng, Yan, Xifeng.* Advances in Neural Information Processing Systems, 2024. [[abs](https://arxiv.org/abs/2302.11520)]
34. **Lima: Less is more for alignment** *Zhou, Chunting, Liu, Pengfei, Xu, Puxin, Iyer, Srinivasan, Sun, Jiao, Mao, Yuning, Ma, Xuezhe, Efrat, Avia, Yu, Ping, Yu, Lili, others.* Advances in Neural Information Processing Systems, 2024. [[abs](https://arxiv.org/abs/2305.11206)]

### 5.3 How to approach AGI Alignments

1. **Ethical and social risks of harm from Language**  arXiv 2021. [[abs](https://arxiv.org/abs/2112.04359)]
2. **Beijing AI Safety International Consensus** *Beijing Academy of Artificial Intelligence.* 2024. [Link](https://news.cgtn.com/news/2024-03-14/VHJhbnNjcmlwdDc3NzI1/index.html)
3. **Counterfactual explanations without opening the black box: Automated decisions and the GDPR** *Wachter, Sandra, Mittelstadt, Brent, Russell, Chris.* Harvard Journal of Law \& Technology, 2017.
4. **Scalable agent alignment via reward modeling: a research direction** *Leike, Jan, Krueger, David, Everitt, Tom, Martic, Miljan, Maini, Vishal, Legg, Shane.* arXiv preprint arXiv:1811.07871, 2018. [[abs](https://arxiv.org/abs/1811.07871)]
5. **Building ethics into artificial intelligence** *Yu, Han, Shen, Zhiqi, Miao, Chunyan, Leung, Cyril, Lesser, Victor R, Yang, Qiang.* Presented at Proceedings of the 27th International Joint Conference on Artificial Intelligence, 2018. [Link](No URL)
6. **Human Compatible: Artificial Intelligence and the Problem of Control** *Russell, Stuart.* Viking, 2019. [Link](https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf)
7. **Responsible artificial intelligence: How to develop and use AI in a responsible way** *Dignum, Virginia.* Springer Nature, 2019. [Link](https://link.springer.com/book/10.1007/978-3-030-30371-6)
8. **Machine ethics: The design and governance of ethical AI and autonomous systems** *Winfield, Alan F, Michael, Katina, Pitt, Jeremy, Evers, Vanessa.* Proceedings of the IEEE, 2019. [Link](https://ieeexplore.ieee.org/document/8662743)
9. **Open problems in cooperative AI** *Dafoe, Allan, Hughes, Edward, Bachrach, Yoram, Collins, Tantum, McKee, Kevin R, Leibo, Joel Z, Larson, Kate, Graepel, Thore.* arXiv preprint arXiv:2012.08630, 2020. [[abs](https://arxiv.org/abs/2012.08630)]
10. **Artificial intelligence, values, and alignment** *Gabriel, Iason.* Minds and Machines, 2020. [[abs](https://arxiv.org/abs/2001.09768)]
11. **Artificial Intelligence Safety and Security** *Yampolskiy, Roman V.* CRC Press, 2020. [Link](https://www.routledge.com/Artificial-Intelligence-Safety-and-Security/Yampolskiy/p/book/9780815369820)
12. **Cooperative AI: machines must learn to find common ground** *Dafoe, Allan, Bachrach, Yoram, Hadfield, Gillian, Horvitz, Eric, Larson, Kate, Graepel, Thore.* arXiv 2021. [[abs](https://arxiv.org/abs/No eprint ID)]
13. **Machine morality, moral progress, and the looming environmental disaster** *Kenward, Ben, Sinclair, Thomas.* arXiv 2021. [[abs](https://arxiv.org/abs/No eprint ID)]
14. **X-risk analysis for ai research** *Hendrycks, Dan, Mazeika, Mantas.* arXiv preprint arXiv:2206.05862, 2022. [[abs](https://arxiv.org/abs/2206.05862)]
15. **Task decomposition for scalable oversight (AGISF Distillation)** *Charbel-Raphaël Segerie.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
16. **Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision** *Burns, Collin, Izmailov, Pavel, Kirchner, Jan Hendrik, Baker, Bowen, Gao, Leo, Aschenbrenner, Leopold, Chen, Yining, Ecoffet, Adrien, Joglekar, Manas, Leike, Jan, others.* arXiv preprint arXiv:2312.09390, 2023. [[abs](https://arxiv.org/abs/2312.09390)]
17. **Whose opinions do language models reflect?** *Santurkar, Shibani, Durmus, Esin, Ladhak, Faisal, Lee, Cinoo, Liang, Percy, Hashimoto, Tatsunori.* Presented at International Conference on Machine Learning, 2023. [Link](No URL)
18. **Ai alignment: A comprehensive survey** *Ji, Jiaming, Qiu, Tianyi, Chen, Boyuan, Zhang, Borong, Lou, Hantao, Wang, Kaile, Duan, Yawen, He, Zhonghao, Zhou, Jiayi, Zhang, Zhaowei, others.* arXiv preprint arXiv:2310.19852, 2023. [[abs](https://arxiv.org/abs/2310.19852)]
19. **Open problems and fundamental limitations of reinforcement learning from human feedback** *Casper, Stephen, Davies, Xander, Shi, Claudia, Gilbert, Thomas Krendl, Scheurer, J\'e.* arXiv preprint arXiv:2307.15217, 2023. [[abs](https://arxiv.org/abs/2307.15217)]
20. **The unlocking spell on base llms: Rethinking alignment via in-context learning** *Lin, Bill Yuchen, Ravichander, Abhilasha, Lu, Ximing, Dziri, Nouha, Sclar, Melanie, Chandu, Khyathi, Bhagavatula, Chandra, Choi, Yejin.* arXiv preprint arXiv:2312.01552, 2023. [[abs](https://arxiv.org/abs/2312.01552)]
21. **Large language model alignment: A survey** *Shen, Tianhao, Jin, Renren, Huang, Yufei, Liu, Chuang, Dong, Weilong, Guo, Zishan, Wu, Xinwei, Liu, Yan, Xiong, Deyi.* arXiv preprint arXiv:2309.15025, 2023. [[abs](https://arxiv.org/abs/2309.15025)]

## 6. Approach AGI Responsibly
### 6.1 AI Levels: Charting the Evolution of Artificial Intelligence
#### 6.1.1 AGI Levels
#### 6.1.2 Constraints and Challenges of Ultimate AGI
#### 6.1.3 How Do We Get to the Next Level of AGI?
### 6.2 AGI Evaluation
#### 6.2.1 What Do We Expect from AGI Evaluations
#### 6.2.2 Current Evaluation Frameworks and Limitations
### 6.3 Potential Ways to Future AGI

## 7. Case Studies
### 7.1 AI for Science Discovery and Research
### 7.2 Generative Visual Intelligence
1. **Deep Unsupervised Learning using Nonequilibrium Thermodynamics**. *Jascha Sohl-Dickstein* et al. ICML 2015. [[paper](https://arxiv.org/abs/1503.03585)]
2. **Generative Modeling by Estimating Gradients of the Data Distribution**. *Yang Song* et al. NeurIPS 2019. [[paper](https://arxiv.org/abs/1907.05600)]
3. **Denoising Diffusion Probabilistic Models**. *Jonathan Ho* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2006.11239)]
4. **Score-Based Generative Modeling through Stochastic Differential Equations**. *Yang Song* et al. ICLR 2021. [[paper](https://arxiv.org/abs/2011.13456)]
5. **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**. *Alex Nichol* et al. arXiv 2021. [[paper](https://arxiv.org/abs/2112.10741)]
6. **SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations**. *Chenlin Meng* et al. ICLR 2022. [[paper](https://arxiv.org/abs/2108.01073)]
7. **Video Diffusion Models**. *Jonathan Ho* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2204.03458)]
8. **Hierarchical Text-Conditional Image Generation with CLIP Latents**. *Aditya Ramesh* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2204.06125)]
9. **Classifier-Free Diffusion Guidance**. *Jonathan Ho* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2207.12598)]
10. **Palette: Image-to-Image Diffusion Models**. *Chitwan Saharia* et al. SIGGRAPH 2022. [[paper](https://arxiv.org/abs/2111.05826)]
11. **High-Resolution Image Synthesis with Latent Diffusion Models**. *Robin Rombach* et al. CVPR 2022. [[paper](https://arxiv.org/abs/2112.10752)]
12. **Adding Conditional Control to Text-to-Image Diffusion Models**. *Lvmin Zhang* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2302.05543)]
13. **Scalable Diffusion Models with Transformers**. *William Peebles* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2212.09748)]
14. **Sequential Modeling Enables Scalable Learning for Large Vision Models**. *Yutong Bai* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.00785)]
15. **Video Generation Models as World Simulators**. *Tim Brooks* et al. OpenAI 2024. [[paper](https://openai.com/research/video-generation-models-as-world-simulators)]

### 7.3 World Models
### 7.4 Decentralized LLM
### 7.5 AI for Coding
### 7.6 AI for Robotics in Real World Applications
### 7.7 Human-AI Collaboration

1. **CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities**
   *Mina Lee, Percy Liang, Qian Yang*. CHI 2022. [[paper](http://arxiv.org/abs/2201.06796)]
2. **A Design Space for Intelligent and Interactive Writing Assistants**
   *Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue*. CHI 2024. [[paper](http://arxiv.org/abs/2403.14117)]
3. **CreativeConnect: Supporting Reference Recombination for Graphic Design Ideation with Generative AI**
   *DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, Juho Kim*
   CHI 2024. [[paper](http://arxiv.org/abs/2312.11949)]
4. **I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence**
   *Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, Bongwon Suh*. CHI 2018. [[paper](https://dl.acm.org/doi/10.1145/3173574.3174223)]
5. **CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs**
   *Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Z. Henley, Paul Denny, Michelle Craig, Tovi Grossman*. CHI 2024. [[paper](http://arxiv.org/abs/2401.11314)]
6. **AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation**
   *Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben Shoshan*. CHI 2024. [[paper](http://arxiv.org/abs/2402.14978)]
7. **Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork**
   *Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, Daniel S. Weld*. AAAI 2021. [[paper](http://arxiv.org/abs/2004.13102)]
8. **Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff**
   *Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S. Weld, Walter S. Lasecki, Eric Horvitz*. AAAI 2019. [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/4087)]
9. **Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems**
   *Qian Yang, Yuexing Hao, Kexin Quan, Stephen Yang, Yiran Zhao, Volodymyr Kuleshov, Fei Wang*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581393)]
10. **AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts**
      *Tongshuang Wu, Michael Terry, Carrie Jun Cai*. CHI 2022. [[paper](https://dl.acm.org/doi/10.1145/3491102.3517582)]
11. **Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows**
    *Madeleine Grunde-McLaughlin, Michelle S. Lam, Ranjay Krishna, Daniel S. Weld, Jeffrey Heer*. arXiv, 2023. [[paper](http://arxiv.org/abs/2312.11681)]
12. **Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts**
    *J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, Qian Yang*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581388)]
13. **Designing Theory-Driven User-Centric Explainable AI**
    *Danding Wang, Qian Yang, Ashraf Abdul, Brian Y. Lim*. CHI 2019. [[paper](https://dl.acm.org/doi/10.1145/3290605.3300831)]
14. **Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning**
    *Krzysztof Z. Gajos, Lena Mamykina*. IUI 2022. [[paper](https://dl.acm.org/doi/10.1145/3490099.3511138)]

## 8. Conclusion


<!-- ### Agent & Tool:
- [Tool learning with foundation models](https://arxiv.org/pdf/2304.08354.pdf)
- [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)
- [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864.pdf)
- [Cognitive Architectures for Language Agents](https://arxiv.org/pdf/2309.02427.pdf)

### (Multi-Modal) LLM / Foundation Model:
#### LLM
- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
- [A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234.pdf)
- [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf)

#### Multi-Modal LLM
- [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf)

#### General Foundation Model
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)

### Embodied AI:
#### AGI
- [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/pdf/2304.06488.pdf)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)
- [Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models](https://arxiv.org/pdf/2306.08641.pdf)
- [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)

### Github:
- [LLM-Agent-Paper-List](https://github.com/WooooDyy/LLM-Agent-Paper-List) (Agent)
- [Awesome-AI-Agents](https://github.com/e2b-dev/awesome-ai-agents) (Agent)
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) (Multi-modal LLM) -->
